# Vizable

Authors: Aracely Moreno, Naila Thevenot, Sarah Branch, Mumtaz Fatima

Advisors: SouYoung Jin, Peyton Greve

### Abstract 
There are many in the US experiencing visual impairements. Our application presents a solution using machine learning to allow people to take an image on their phone and populate a caption that is converted to the language on their phone system. We use the VizWiz dataset, a collection of images taken by people who are blind and OpenAI's CLIP model. 

<img width= "500" src="https://user-images.githubusercontent.com/63077056/213947004-da058f4d-7ff3-459a-a5c9-7efc0ef9f241.png" alt="image">


<img alt = "image" width= "500" src = "https://user-images.githubusercontent.com/63077056/213947012-f8b41ab0-e8f2-4c31-96ab-b0ba43e6ae0a.png">

<img alt = "image" width= "500" src = "https://user-images.githubusercontent.com/63077056/213947019-7c17068b-e841-498e-9e09-d4bd458627ba.png">

<img alt = "image" width= "500" src = "https://user-images.githubusercontent.com/63077056/213947027-65eda1b2-9143-40bd-a0bd-1aac42092839.png">

<img alt = "image" width= "500" src = "https://user-images.githubusercontent.com/63077056/213947032-c0205f63-1ff6-440b-b4e9-f7d33c371927.png">

<img width="300" alt="image" src="https://user-images.githubusercontent.com/63077056/213946814-0ee4e5fc-49cd-43d8-b0e7-2de247d86e6b.png">


Demo: https://youtu.be/CPAI3SdoC1Q

